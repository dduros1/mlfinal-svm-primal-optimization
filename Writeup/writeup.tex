\documentclass[letterpaper, 11pt]{article}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{times}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{cite}

\begin{document}

\title{Comparison of SVM Optimazation Techniques in the Primal}
\author{Diane Duros and Jonathan Katzman}
\maketitle
\begin{abstract}This paper examines the efficacy of different optimzation techniques in a primal formulation of a support vector machine (SVM).  Three main techniques are compared.  The benchmark to compare all three techniques was sentiment analysis on movie reviews.
\end{abstract}
                                                                                                                                                                                                                                                                           
\section{Introduction}


\section{Data}
We retrieved our data from the problem "Sentiment Analysis on Movie Reviews," from kaggle.com.  The data originates from the Rotten Tomatoes dataset and consists of phrases that have been assigned sentiment labels, where the sentiment labels are:
\begin{description}
	\item[0] negative
	\item[1] somewhat negative
	\item[2] neutral
	\item[3] somewhat positive
	\item[4] positive
\end{description}

Initially, we collapsed the labels to binary labels, where an original label of 3 or 4 was considered positive, and 2 or below became a negative label.  

Talk about options for data

\subsection{Features}
We generated features based on a bag of words model

\section{SVMs}

\textbf{Basis} Yay it workses

\subsection{Multiclass SVM}

\section{Primal Optimization Methods}

\subsection{Gradient Descent}

\subsection{Newton's Approximation}

\textbf{RBF Kernel}

Large sigma values produced poor results (on a pairwise SVM, accuracies were less than .5), but terminated quickly.  However, we did not have any terminating runs with small sigma values. <Hopefully until Monday night when we write more about this! TODO>


\subsection{Stochastic Subgradient}

\textbf{Initialization}


\section{Results}
Things to discuss:
\begin{itemize}
	\item Variations in parameters for each method
	\item Binary vs multi class accuracy AND timing
	\item timing
	\item convergence
\end{itemize}

\subsection{Crossvalidation Results}


\subsetion{Kaggle.com Results}

\section{Conclusion}



\nocite{*}
\bibliographystyle{plain} 
\bibliography{writeup}

\end{document}